{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from random import randrange\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from Game import Game\n",
    "from copy import deepcopy\n",
    "from QLearning import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(display=False, seed=123, dist_func='euclid', exp_func='eps-greedy', num_episodes=15, csv_name='training_data'):\n",
    "    # Initialize game and learner\n",
    "    game = Game(display=display, random_seed=seed)\n",
    "    learner = QLearning(game, dist_func=dist_func, exp_func=exp_func)\n",
    "\n",
    "    # Display run parameters\n",
    "    print(\"\"\"Running game with:\n",
    "                        Seed: {}\n",
    "                        Distance Function: {}\n",
    "                        Exploration Function: {}\n",
    "                        # of Episodes: {}\n",
    "                        CSV Filename: {}\n",
    "                \"\"\".format(seed, dist_func, exp_func, num_episodes,csv_name))\n",
    "\n",
    "    # Initialize data structure for CSV and list of minimal actions\n",
    "    training_data = []\n",
    "    minimal_actions = game.ale.getMinimalActionSet()\n",
    "    minimal_actions.pop(1)\n",
    "\n",
    "    # Start training\n",
    "    for episode in range(num_episodes):\n",
    "        # Initialize reward\n",
    "        total_reward = 0\n",
    "        count = 0 \n",
    "\n",
    "        game.initialize()\n",
    "        while not game.is_over():\n",
    "            # Get current state q_values and grad_theta_q values\n",
    "            curr_state_q = learner.q_func(game)[1]\n",
    "            curr_state_fevals = np.array(learner.get_distances(game))\n",
    "            \n",
    "            # Get action based on exploration strategy\n",
    "            if learner.exp_func == \"eps-greedy\" and np.random.random() < learner.eps:\n",
    "                best_action = learner.get_eps_greedy_action()\n",
    "            elif learner.exp_func == \"softmax\":\n",
    "                best_action = learner.get_softmax_action()\n",
    "            else:\n",
    "                best_action = learner.get_max_q_action()\n",
    "\n",
    "#             print(\"Before:\")\n",
    "#             plot(game)\n",
    "            \n",
    "            # Execute action and update weights based on reward\n",
    "            reward = game.ale.act(best_action[0])   \n",
    "            game.update_RAM()\n",
    "            reward += game.get_reward()\n",
    "            \n",
    "            learner.update_weights(curr_state_q, curr_state_fevals, best_action, reward)\n",
    "            \n",
    "#             print(\"After action {}:\".format(str(best_action[0])))\n",
    "#             plot(game)\n",
    "#             print(\"Reward: %d\" % reward)\n",
    "#             print(\"Qbert Pos: {}\".format(game.player.pos))\n",
    "#             print (\"RAM : {}\".format(game.RAM))\n",
    "            \n",
    "            total_reward += reward\n",
    "            count += 1\n",
    "            \n",
    "        plot(game)\n",
    "        print(learner.weights)\n",
    "        print(\"Episode %d ended with score: %d\" % (episode, total_reward))\n",
    "        \n",
    "        # Append data to array for CSV writing\n",
    "        final_values = [episode, total_reward] + list(learner.weights)\n",
    "        training_data.append(final_values)\n",
    "        \n",
    "        game.reset()\n",
    "        \n",
    "    return game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(game):\n",
    "    game.ale.getScreenRGB(game.screen)\n",
    "    plt.imshow(game.screen)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "g = main(num_episodes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_states = [[0], \n",
    "                [0,0],\n",
    "                [0,0,0],\n",
    "                [0,0,0,0],\n",
    "                [0,0,0,0,0],\n",
    "                [0,0,0,0,0,0]]\n",
    "\n",
    "BLOCK_POS = [[(76,35)], \n",
    "            [(64,63),(92,63)],\n",
    "            [(53,92),(77,92),(104,92)],\n",
    "            [(40,121),(64,121),(92,121),(117,121)],\n",
    "            [(29,150),(52,150),(76,150),(105,150),(128,150)],\n",
    "            [(16,179),(40,179),(64,179),(93,179),(116,179),(140,179)]]\n",
    "\n",
    "DISC_POS = [(15,138), (144,138)]\n",
    "\n",
    "# block_states = [[0], \n",
    "#                 [1,1],\n",
    "#                 [1,1,1],\n",
    "#                 [1,1,1,1],\n",
    "#                 [1,1,1,1,1],\n",
    "#                 [1,1,1,1,1,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_pos = np.array((76,35))\n",
    "disc_pos = np.array((140,179))\n",
    "# disc_pos = np.array(DISC_POS[0])\n",
    "np.sum(np.fabs(disc_pos - q_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2576.0\n"
     ]
    }
   ],
   "source": [
    "# Max distance of blocks\n",
    "dist = 0\n",
    "player_pos = (76, 35)\n",
    "row_num = 0\n",
    "for row in block_states:\n",
    "    block_num = 0            \n",
    "    for block_state in row:\n",
    "        if block_state == 0:\n",
    "            q_pos = np.array(player_pos)\n",
    "            block_pos = np.array(BLOCK_POS[row_num][block_num])\n",
    "            dist += np.sum(np.fabs(block_pos - q_pos))\n",
    "        block_num += 1\n",
    "    row_num += 1\n",
    "\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.floor(2 ** 3.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
