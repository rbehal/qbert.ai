{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from random import randrange\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from Game import Game\n",
    "from copy import deepcopy\n",
    "from QLearning import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(display=False, seed=123, dist_func='euclid', exp_func='eps-greedy', num_episodes=15, csv_name='training_data'):\n",
    "    # Initialize game and learner\n",
    "    game = Game(display=display, random_seed=seed)\n",
    "    learner = QLearning(game, dist_func=dist_func, exp_func=exp_func)\n",
    "\n",
    "    # Display run parameters\n",
    "    print(\"\"\"Running game with:\n",
    "                        Seed: {}\n",
    "                        Distance Function: {}\n",
    "                        Exploration Function: {}\n",
    "                        # of Episodes: {}\n",
    "                        CSV Filename: {}\n",
    "                \"\"\".format(seed, dist_func, exp_func, num_episodes,csv_name))\n",
    "\n",
    "    # Initialize data structure for CSV and list of minimal actions\n",
    "    training_data = []\n",
    "    minimal_actions = game.ale.getMinimalActionSet()\n",
    "    minimal_actions.pop(1)\n",
    "\n",
    "    # Start training\n",
    "    for episode in range(num_episodes):\n",
    "        # Initialize reward\n",
    "        total_reward = 0\n",
    "        count = 0 \n",
    "\n",
    "        game.initialize()\n",
    "        while not game.is_over():\n",
    "            # Get current state q_values and grad_theta_q values\n",
    "            curr_state_q = learner.q_func(game)[1]\n",
    "            curr_state_fevals = np.array(learner.get_distances(game))\n",
    "            \n",
    "            # Get action based on exploration strategy\n",
    "            if learner.exp_func == \"eps-greedy\" and np.random.random() < learner.eps:\n",
    "                best_action = learner.get_eps_greedy_action()\n",
    "            elif learner.exp_func == \"softmax\":\n",
    "                best_action = learner.get_softmax_action()\n",
    "            else:\n",
    "                best_action = learner.get_max_q_action()\n",
    "\n",
    "            print(\"Before:\")\n",
    "            plot(game)\n",
    "            \n",
    "            # Execute action and update weights based on reward\n",
    "            reward = game.ale.act(best_action[0])   \n",
    "            game.update_RAM()\n",
    "            reward += game.get_reward()\n",
    "            \n",
    "            print(\"After:\")\n",
    "            plot(game)\n",
    "            \n",
    "            learner.update_weights(curr_state_q, curr_state_fevals, best_action, reward)\n",
    "\n",
    "            total_reward += reward\n",
    "            count += 1\n",
    "            \n",
    "        print(learner.weights)\n",
    "        print(\"Episode %d ended with score: %d\" % (episode, total_reward))\n",
    "        \n",
    "        # Append data to array for CSV writing\n",
    "        final_values = [episode, total_reward] + list(learner.weights)\n",
    "        training_data.append(final_values)\n",
    "        \n",
    "        game.reset()\n",
    "        \n",
    "    return game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "g = main(num_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot(game):\n",
    "    game.ale.getScreenRGB(game.screen)\n",
    "    plt.imshow(game.screen)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ep_num = 10\n",
    "\n",
    "for i in range(len(a[ep_num])):\n",
    "    action = a[ep_num][i][1].split(\".\")[1]\n",
    "    \n",
    "    print(\"Before action: %s\" % action)\n",
    "    plt.imshow(a[ep_num][i][0])\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"After action: %s\" % action)\n",
    "    plt.imshow(b[ep_num][i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nonzero(a[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game.ale.getRAM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.ale.getMinimalActionSet()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
