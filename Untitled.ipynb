{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from random import randrange\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from Game import Game\n",
    "from copy import deepcopy\n",
    "from QLearning import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(display=False, seed=123, dist_func='euclid', exp_func='eps-greedy', num_episodes=15, csv_name='training_data'):\n",
    "    # Initialize game and learner\n",
    "    game = Game(display=display, random_seed=seed)\n",
    "    learner = QLearning(game, dist_func=dist_func, exp_func=exp_func)\n",
    "\n",
    "    # Display run parameters\n",
    "    print(\"\"\"Running game with:\n",
    "                        Seed: {}\n",
    "                        Distance Function: {}\n",
    "                        Exploration Function: {}\n",
    "                        # of Episodes: {}\n",
    "                        CSV Filename: {}\n",
    "                \"\"\".format(seed, dist_func, exp_func, num_episodes,csv_name))\n",
    "\n",
    "    # Initialize data structure for CSV and list of minimal actions\n",
    "    training_data = []\n",
    "    minimal_actions = game.ale.getMinimalActionSet()\n",
    "    \n",
    "    # Testing variables #\n",
    "    before_action_d = {}\n",
    "    after_action_d = {}\n",
    "\n",
    "    # Start training\n",
    "    for episode in range(num_episodes):\n",
    "        # Initialize reward\n",
    "        total_reward = 0\n",
    "        count = 0 \n",
    "        action_taken = False\n",
    "\n",
    "        # Testing variables #\n",
    "        before_action_a = []\n",
    "        after_action_a = []\n",
    "        \n",
    "        while not game.is_over():\n",
    "            # Get current state q_values and grad_theta_q values\n",
    "            curr_state_q = learner.q_func(game)[1]\n",
    "            curr_state_fevals = np.array(learner.get_distances(game))\n",
    "\n",
    "            # Get action based on exploration strategy\n",
    "            if learner.exp_func == \"eps-greedy\" and np.random.random() < learner.eps:\n",
    "                best_action = learner.get_eps_greedy_action()\n",
    "            elif learner.exp_func == \"softmax\":\n",
    "                best_action = learner.get_softmax_action()\n",
    "            else:\n",
    "                best_action = learner.get_max_q_action()\n",
    "\n",
    "            game.ale.getScreenRGB(game.screen)\n",
    "            before_action_a.append((deepcopy(game.screen), str(best_action[0])))\n",
    "\n",
    "            reward = game.ale.act(best_action[0])\n",
    "            game.update_RAM()\n",
    "\n",
    "            count = 0\n",
    "            while not (game.RAM[0] == 0):\n",
    "                count += 1\n",
    "                \n",
    "                score_diff = game.ale.act(0)\n",
    "                if score_diff != 0:\n",
    "                    print(score_diff)\n",
    "                game.update_RAM()\n",
    "                \n",
    "                if count % 100 = 0:\n",
    "                    plt.imshow(game.screen)\n",
    "                    plt.show()\n",
    "                \n",
    "                    print(game.RAM)\n",
    "            \n",
    "            after_action_a.append(deepcopy(game.screen))\n",
    "            # Execute action and update weights based on reward\n",
    "            learner.update_weights(curr_state_q, curr_state_fevals, best_action, reward)\n",
    "\n",
    "            total_reward += reward\n",
    "                \n",
    "            count += 1\n",
    "            \n",
    "        print(learner.weights)\n",
    "        print(\"Episode %d ended with score: %d\" % (episode, total_reward))\n",
    "        \n",
    "        # Append data to array for CSV writing\n",
    "        final_values = [episode, total_reward] + list(learner.weights)\n",
    "        training_data.append(final_values)\n",
    "\n",
    "        before_action_d[episode] = before_action_a\n",
    "        after_action_d[episode] = after_action_a\n",
    " \n",
    "        \n",
    "        game.reset()\n",
    "        \n",
    "    return before_action_d, after_action_d, game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c = main(num_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0, 148, 184, 148, 212, 148,   0, 148, 254, 148, 166,   3, 176,\n",
       "         3, 176,   3, 176,   3, 176,   3, 176, 148,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0, 148,  64, 148,   5, 148,   0, 148,\n",
       "         0, 148,  59, 148,  77,  65,  53,  93, 105,  93,   0,   0, 189,\n",
       "       148, 102, 148,   0, 254,   0,  21,   0,  16, 224,  48,  96,  48,\n",
       "         6,   0, 254, 254,   5,  13,   5,   0,   0,  32,   4,   3,   6,\n",
       "         7,   6,   0,   0,   0, 148,   0, 148,   0, 148,   0,   0,   0,\n",
       "         0,   7,   7,   7,   7,   7, 191, 148,   0, 148, 127, 148,  15,\n",
       "       148,   1,   1,   4,  68,  68,  68,   4, 210,   3,  20,   2,   1,\n",
       "         0,   1,  36,  21,   0, 210,  13, 217, 188, 201, 180], dtype=uint8)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(c.screen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ep_num = 10\n",
    "\n",
    "for i in range(len(a[ep_num])):\n",
    "    action = a[ep_num][i][1].split(\".\")[1]\n",
    "    \n",
    "    print(\"Before action: %s\" % action)\n",
    "    plt.imshow(a[ep_num][i][0])\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"After action: %s\" % action)\n",
    "    plt.imshow(b[ep_num][i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nonzero(a[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game.ale.getRAM()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
